\documentclass{article}

\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}

\title{Learning to Transfer with Triply Adversarial Nets}


\author{
Gilles Louppe \\
New York University\\
\texttt{g.louppe@nyu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}

In classification, transfer learning (or its variants known as co-variate shift
or domain adaptation) arises whenever target instances are governed by a
distribution that may be arbitrarily different from the distribution of the
source instances used at training. This problem has traditionally been solved by re-weighting
approaches or by learning robust representations over domains. In this work, we
propose a new paradigm based on the assumption that the co-variate shift is only
due to a different representation of the same underlying objects.
Accordingly, we propose to learn how to transform source instances into target
instances, possibly across input spaces of distinct dimensions, structures or
supports. For this purpose, we extend the generative adversarial networks
framework of \cite{goodfellow2014generative} to a triply adversarial process: a
transformer network $G$ for generating target instances from source instances, a
discriminative network $D$ for separating transformed source instances from
actual target instances, and a classifier network $C \circ G$ for classifying source
instances in the projected space. This 3-player game results in a network $G$
capable of transforming source into target instances, while preserving
separation between classes as enabled by $C$ in the adversarial setup.
Preliminary experiments demonstrate the potential of this novel approach, with promising
results when the construction of $C$ can be bootstrapped in a semi-supervised
way  from a few labeled instances from the target space.

\end{abstract}

\section{Introduction}

\glnote{to write}

% - transfer learning has traditionally been solved through density ratio reweighting
% - works fine, but has the big issue of not working if the support of the source density does not cover the target density support\
%   support is needed for reweighting, but not here (Gretton et al) we can even have different space

% - in this work, we propose instead a new paradigm for solving transfer learning
% - build a generative model for transforming samples from p1 to reproduce p0
% - adversarial learning from Goodfellow, with the practical difference that we often have only a finite set of samples (we need to avoid learning a lookup table)

% other view point: start from dual adversaries, then add to the mix C is a regularizer
% - ensure p(y|t(x)) ~ p(y|x) by having a third classifier in the mix
% issue: T is not unique, how make sure samples are mapped correctly?

% - application: fix simulated data

% Ganin: domains share the same input space, we dont necessarily have to
%        seek a common robust representation, here we seek instead to transform training into test

\section{Problem statement}

Let assume a probability space $(\Omega, {\cal F}, P)$, where $\Omega$ is a
sample space, ${\cal F}$ is a set of events and $P$ is a probability measure.
Let consider a (multivariate) random variable $X: \Omega \mapsto
\mathbb{R}^p$ inducing the source distribution $p_X$, along with a
finite set $\{x_i\}_{i=1}^N$ of its realizations $X(\omega_i)$, for $\omega_i
\in \Omega$.
In classification,
let us further assume that realizations from the source distribution are extended with realizations $\{y_i\}_{i=1}^N$
of a label random variable $Y:\Omega \mapsto {\cal Y}$, thereby inducing the joint source distribution $p_{X,Y}$.
Similarly, let assume a (multivariate) random variable $U: \Omega
\mapsto \mathbb{R}^q$ inducing the target distribution $p_U$, along with a finite set
$\{u_j\}_{j=1}^M$ of its realizations $U(\omega_j)$, for $\omega_j \in \Omega$.
Assuming it exists, our goal is to find a transfer function $T: \mathbb{R}^p \times {\cal Y}
\mapsto \mathbb{R}^q$ such that
\begin{equation}\label{eqn:tf}
T(X(\omega), Y(\omega)) = U(\omega) ~\text{for all}~ \omega \in \Omega.
\end{equation}

Since we do not have training tuples $((X(\omega), Y(\omega)), U(\omega))$ (for the same
unknown $\omega$) from which $T$ could be learned using standard regression
algorithms, we propose instead to solve the closely related problem of
finding a transfer function $\hat T$ such that
\begin{equation}\label{eqn:tf-proxy}
P(\{ \omega | \hat T(X(\omega), Y(\omega)) = u \}) = P(\{ \omega' | U(\omega') = u \}) ~\text{for all}~ u \in \mathbb{R}^q.
\end{equation}
In words, we are looking for a transfer function $\hat T$ such that realizations
of $\hat T(X,Y)$ are indistinguishable from realizations of $U$. A function $T$ for
which Eqn.~\ref{eqn:tf} is true necessarily satisfies Eqn.~\ref{eqn:tf-proxy}.
The converse is however in general not true, since the sets of events $\{ \omega |
\hat T(X(\omega), Y(\omega)) = u \}$ and $\{ \omega' | U(\omega') = u \}$ do not need to be
the same for the equality to hold. Accordingly, the contribution of this work is
to propose a procedure for constructing transfer functions satisfying
Eqn.~\ref{eqn:tf-proxy}, and for which Eqn.~\ref{eqn:tf} is {\it plausibly}
satisfied.

In these terms, our framework encompasses the problems of transfer learning,
domain adaptation or co-variate shift, where the labeled training samples
correspond to the labeled realizations of the source distribution and where the
unlabeled test samples correspond to realizations of the target distribution. At
its core, this framework assumes that the underlying objects $\omega$ share a
same universe $\Omega$ and that the source and target distributions only differ
in the way they represent these objects. While not verified in all cases, we
believe this assumption to be met in many practical situations, e.g., when
learning to transfer across natural images from distinct datasets but
representing the same concepts or when learning to adapt to a change of a
measurement apparatus (but not of the underlying objects to be observed).

\glnote{What are the necessary conditions on $p_{X,Y}$ and $p_U$ for
        the existence of $T$? and of $\hat T$? I think results can be proven using information theory.}

% XXX: unsupervised learning


\section{Method}

Generative adversarial networks (GAN) were first proposed by
\cite{goodfellow2014generative} as a way to generate samples from random noise
$z \sim p_Z$. In this work, the authors pit a generative model $G$ against an
adversary classifier $D$ whose repelling objective is to recognize real from
fake samples. Both models $G$ and $D$ are trained simultaneously, in such a way
that $G$ learns to produce samples that are difficult to classify by $D$, while
$D$ incrementally adapts to changes in $G$. At the equilibrium, $G$ models a
distribution whose fake samples are recognized by $D$ only by chance. In other
words, assuming enough capacity in $D$, the distribution $p_{G(Z)}$ of fake
samples converges towards the distribution of real samples.

In this work, we extend the GAN framework by first noticing that if we replace
the noise distribution $p_Z$ by the joint source distribution $p_{X,Y}$ and consider
the real data distribution as the target distribution $p_U$, then the generative
model $G$ is a transfer function $\hat T : \mathbb{R}^p \times {\cal Y} \mapsto \mathbb{R}^q$
satisfying Eqn.~\ref{eqn:tf-proxy}. That is, generative adversarial networks
provide a direct way for learning to transfer.

\begin{theorem}
At the equilibrium, $G$ satisfies Eqn.~\ref{eqn:tf-proxy}. \glnote{This seems a direct consequence of the convergence results of \cite{goodfellow2014generative}. Needs to be double-checked.}
\end{theorem}

As noted earlier, transfer functions $\hat T$ satisfying Eqn.~\ref{eqn:tf-proxy}
are not unique. It is therefore critical to guide
the adversarial training process in order to obtain a plausible transfer
function. Of special interest in the context of classification
are functions $\hat T$  preserving the conditional class distribution in the target space,
i.e. such that
\begin{equation}\label{eqn:preservation}
    p_{Y|X}(y|x) = p_{Y|\hat T(X, Y)}(y|\hat T(x, y)) ~\text{for all}~ x, y \in \mathbb{R}^p \times {\cal Y}.
\end{equation}
In words, we are looking for a transfer function $\hat T$ such that realizations
of $\hat T(X,Y)$ are indistinguishable from realizations of $U$, while preserving
the class distribution and separability in the target space.
In order to satisfy this constraint, we extend the GAN framework by
adding a third network in the pit: a classifier $C$ whose objective is to classify
labeled source samples in the projected space.
Accordingly, we extend $G$'s objective function so as to maximize $C$'s accuracy,
while still fooling $D$. Formally, assuming binary classification (i.e. ${\cal Y} = \{0, 1\}$), the three networks $D$, $C$ and $G$ are
concurrently trained so as to minimize the respective loss functions:

\begin{align}
    L(D) &= -\mathbb{E}_{u \sim p_U} [\log(1 - D(u))] -\mathbb{E}_{x,y \sim p_{X,Y}} [\log(D(G(x, y)))] \\
    L(C) &= -\mathbb{E}_{x \sim p_{X|Y=0}} [\log(1 - C(G(x, y=0)))] -\mathbb{E}_{x \sim p_{X|Y=1}} [\log(C(G(x, y=1)))] \\
    L(G) &= \mathbb{E}_{x,y \sim p_{X,Y}} [\log(D(G(x, y)))] + L(C)
\end{align}

This 3-player game results in a network $G$
capable of transforming source into target instances, while preserving
separation between classes as enabled by $C$ in the adversarial setup.
Accordingly, when the transformed source distribution equals the
target distribution, $C$ converges towards a classifier that can be used
for classifying samples from the target space, thereby providing a solution
to transfer learning for classification.

\glnote{Add figure}

Despite preserving class proportions and separability in the target space, the
proposed framework does not guarantee that the learned classifier $C$ assigns
the correct labels $Y(\omega)$ to realizations $U(\omega)$. For instance, when
$P(Y=0) = P(Y=1)$, the loss function $L(C)$ is equally minimized when the
correct class is always perfectly predicted, i.e. when $C(G(x,y=0)) = 0$ and
$C(G(x,y=1)) = 1$ for all $x$, than when the incorrect class is always wrongly
predicted,  i.e. when $C(G(x,y=0)) = 1$ and $C(G(x,y=1)) = 0$ for all $x$.
Fortunately, this problem can be mitigated in the common case of
semi-supervised learning, i.e. when at least a few pairs of {\it seed}
realizations $U(\omega), Y(\omega)$ are known in the target space. Indeed, these
seed samples can be used as additional training data when learning $C$, thereby
indirectly constraining the learning of $G$ towards a transfer function for
which source instances projected close to the seed samples should share the same
label, hence yielding a plausible transfer function. Formally,
the loss function $L(C)$ is replaced with
\begin{align}\label{eqn:loss-C-semi}
    L(C) =&~ \gamma (-\mathbb{E}_{x \sim p_{X|Y=0}} [\log(1 - C(G(x, y=0)))] -\mathbb{E}_{x \sim p_{X|Y=1}} [\log(C(G(x, y=1)))]) + \nonumber \\
          & (1 - \gamma) (-\mathbb{E}_{u \sim p_{U|Y=0}} [\log(1 - C(u)] -\mathbb{E}_{x \sim p_{U|Y=1}} [\log(C(u))])
\end{align}
where $\gamma$ is a parameter trading off the weight of incorrect predictions for seed samples
and where expectations $\mathbb{E}_{u \sim p_{U|Y}}$ are approximated empirically
over the known realizations in the target space.


\section{Experiments}

\glnote{To do. See notebooks for a working proof of concept. }
\glnote{Comment on network architecture depending on the kind of transfer.}

% - regularization
% - network architecture (propagate x, similar to highway networks)

\section{Related work}

\glnote{to write}

% also related to multi-view learning

\section{Conclusions}

\glnote{to write}

\subsection*{Acknowledgments}

\glnote{todo}

\bibliographystyle{plain}
\bibliography{bibliography.bib}

\end{document}
